ETCD 可以在键中添加/，这意味着可以使用目录的形式存储数据，并使用prefix方便查询



TTL：

可以设置一个有效期

CAS 对key进行赋值时，需要提供一些条件，条件满足时才能复制成功。



Raft协议：

基于quorum机制，即大多数同意原则，任何的变更都需超过半数的成员确认。



客户端发请求给服务端后，先做一致性的协商；

把请求发给其他的服务器，并保存到一个临时模块，等待其他的模块返回，等待超过半数返回之后，就完成更新；



在分布式系统中，一个node分为三种状态：Follower/Candidate/Leader



在系统中，所有节点初始化时都是Follower，如果follower，如果一段时间内没有指示，就会把自己变成候选人，只要其他的角色没有leader并且有投票的请求，只要半数的node，那么候选人就转为Leader，这个过程叫选举（election）。

当外部有记录写入时，那么就会把这个数据会和心跳一起发送给其他follower，超过半数接受到时，就把这个数据commit；

如果大家一起变成候选人，就会有冲突，这时候就有一个选举超时，这是一个随机值，时间短的会首先被选举，这个时候就重置其他人的超时时间，leader的心跳也会重置这个时间，所有的数据都跟着时间走；

当一个leader停止后，那么其他的follower重新发起选举，新leader比老leader有话语权；

如果是偶数个node，那么就容易产生两个候选者， 这时候会重新选举；

当如果有脑裂情况，那么旧Leader的数据会回滚没有commit的数据，并同步新Leader的数据

4.2.1 版本的新角色learner

当一个节点故障，换一个新的节点，新的节点是没有数据的，那么主节点会把大量数据传给新节点，可能导致带宽耗尽，进而影响心跳，导致集群不可用。

Learner只接受数据，不参与投票，因此增加learner后，集群的quorum不变；



wal日志

wal日志是二进制的，数据结构是LogEntry。

其中Type字段，0表示Normal，1表示ConfChange

第二个字段是term，表示每个结点的任期

第三个字段是index，严格有序递增，代表变更序号；

第四个字段是二进制的data



ETCD V3 Store

KVStore: kvindex 索引、backend-blotdb、lessor存储有效期



在ETCD是支持多版本的，通过revision来实现；

存储一个值的过程：

1）预检查：配额，限速，鉴权，包大小

2）KVServer:把数据发送到一致性模块，一致性有一个raftLog，然后返回一个Ready给KVS，然后把请求发给Leader

3）KVS把数据发送到Follower，Leader同时把信息写到wal日志

4）其他Follower同时写入wal日志，当半数确认好后，那么会把数据从unstable移到committed

5）最后写入到MVCC模块

他会写入两个地方：一个是treeindex ，他存储的也是kv，k就是存储的key，v是一个修改的历史变动信息和当前信息

接下来会写入BoltDB，他是一个B+树，他的key是版本号，把变更的整个信息都记录下来



因为需要一致性，所以需要一个wal日志的中间模块，而MVCC是最终的状态机；



revision的版本号会非常大；

K8S yaml Spec的resourceVersion其实就是ETCD的revision



经过半数确认的就是有效数据

通过revision，可以读取以前的版本



watch以及过期存储

watch机制支持某个固定的key，也支持一个范围

watchGroup有两种，key watcher和range watcher结构体不同；

每个watchstore包含两种WatchGroup，一种synced,一种unsynced; 前者表示已经同步完毕，后者表示还在追赶；



当etcd收到客户端的watch请求，如果请求携带了revison，那个会把revison和store当前revision比较，如果大于，放入synced，如果小于先放到unsynced，读取后再放到synced



ETCD 的重要参数

name

data-dir：数据往哪里存，snap和wal

listen-peer-urls 协商用的

listen-client-urls 客户端

集群相关的参数

-inital-advertise-peer-urls

安全相关的参数，TLS



灾备

创建snapshot，可以把当前数据存放到snap

然后通过命令恢复

单个对象不建议超过1.5M

默认2G

不建议超过8G：boltDB会把数据映射到内存。

Alarm& Disarm Alarm

如果ETCD磁盘爆了，能够get，任何set都会失败，会变成readonly，这时候查看member，会提示alarm:NOSPACE;

这时候使用defrag清理磁盘空间，然后alarm disarm

或者使用 compact 命令，现在的版本有auto compact

一个集群大概5Knode，10-20w的pod，好几千个ns，大概七八百M的样子；



高可用的ETCD解决方案

etcd-operator 基于K8S CRD完成etcd集群配置，搭集群有多种方式，已经有K8S集群，再这个集群上再搭建etcd集群；

bitnami 基于statefulset的一种方案，



Etcd Operator

etcdCluster 会被operator 控制器watch，当监控到之后会生成一个etcd pod，pod有两部分，etcd和backup，etcd会挂载数据到local volume



首先判断操作类型，是add还是modified还是delete

如果是add，那么会新开启一个线程，去判断是不是应该新建一个集群

如果集群状态为空，那么就新建一个集群，更新一下状态，创建一个新的seedMember，设置一些相关的参数，然后调用client的create pod



### kubernetes如何使用ETCD？

对于每一个kubernetes Object，都有一个storage负责存储操作。

API Server：

指定etcd的客户端和TLS认证证书；

现在是调用etcd的api 获取call；

有人写控制器写了很多请求对象到api-server；



分离：

来通过etcd-servers-overrides分离可以写的数据给其他etcd

生产中通过堆叠的方式搭建，和api server绑定在同一个集群里面；

外部ETCD搭建：解耦



生产上的考虑：

多个peer合适？

1个太少，不安全

3个性能好，和5个比较起来，如果1个ETCD坏了，需要立马有人处理，否则第2个坏了之后，会导致事故；

5个牺牲了性能，但是容错率会变高；

保证高可用是首要目标

不需要动态扩容



保证apiserver和etcd之间的高效性通讯

apiserver和etcd之间的通讯基于gRPC

针对每个object，apiserver 和etcd之间的connection -> stream 共享

有http2的特性

​	stream quota 

​	带来的问题？对于大规模集群，会造成链路阻塞

​	10000个pod，一次list操作需要返回的数据可能超过100M





etcd 存储规划

​	remote storage 

​		优势是永远可用，但现实并不是如此，可能会阻塞

​		劣势是IO效率

​	最佳时间：local 专有 ssd，利用local volume分配空间

多少空间？

​	针对8g进行告警

​	与集群规模相关，为什么DB size不一致

​	wallog日志占了空间





安全性：

​	peer和peer通讯加密

​	数据加密

​	事件分离



减少网络延迟

​	ETCD同地部署加备份



减少磁盘IO



保持合理的日志文件大小

 wallog日志文件默认是10000条



设置合理的存储配额



自动压缩历史版本



清理碎片



etcd备份

​	etcd备份：备份完整的集群信息，

​	etcdctl snapshot save



频度

​	定期snapshot 

​	定期把wallog挖出来

​	

优化运行参数

​	默认心跳100ms

​	默认选举1000ms

​	

常见问题

watch transfer-encoding chunked

磁盘暴涨



少数etcd 成员down

apiserver增强，不仅端口检查，还调用api接口，出了问题后apiserver自杀



Master节点出现网络分区

apiserver尝试是否可以写，如果有问题就自杀



## ETCD及其特点



## ETCD适应的场景
